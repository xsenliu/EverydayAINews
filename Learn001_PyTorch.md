[Deep Learning with PyTorch: A 60 Minute Blitz](https://docs.pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)
<details>
  <summary>ç‚¹å‡»å±•å¼€ä»£ç å—</summary>

import torch
import numpy as np

#init
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)
print(x_data)

#np array
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
print(x_np)

#like
x_ones = torch.ones_like(x_data) # retains the properties of x_data
print(f"Ones like Tensor: \n {x_ones} \n")
#like and override
x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data
print(f"Random like Tensor: \n {x_rand} \n")

#shape
shape = (2, 3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print("shape:")
print(f"Random Tensor: \n {rand_tensor} ")
print(f"Ones Tensor: \n {ones_tensor} ")
print(f"Zeros Tensor: \n {zeros_tensor}")

</details>

# 1120 
MLP + éšè—å±‚ + æ¿€æ´»å‡½æ•° + æ¢¯åº¦ä¸‹é™ + Adam ä¼˜åŒ–å™¨
[copilot:learn pytorch](https://copilot.microsoft.com/shares/gs2Dy6e3kvTPnzUwkvcjk)
## åŸºæœ¬æ¦‚å¿µ
- å‰å‘ä¼ æ’­ (Forward pass)ï¼šè¾“å…¥ã€‹å¼ é‡è¿ç®—ã€‹è¾“å‡º
- æŸå¤±å‡½æ•°ï¼šæ¨¡å‹è¾“å‡ºå’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼Œæ¯”å¦‚å‡æ–¹è¯¯å·® (MSE)ã€äº¤å‰ç†µ (Cross-Entropy)ã€‚
  > æŸå¤±å‡½æ•° ğ¿ æ˜¯å‚æ•°çš„å‡½æ•°ï¼š
  > ğ¿=ğ‘“(ğ‘Š,ğ‘,ğ‘¥)
- æ¢¯åº¦ï¼šæŸå¤±å‡½æ•°å¯¹å‚æ•°çš„åå¯¼æ•°ã€‚å³ï¼šå¦‚æœå‚æ•°ğ‘Šæ”¹å˜ä¸€ç‚¹ç‚¹ï¼ŒæŸå¤±ğ¿ä¼šæ€ä¹ˆå˜åŒ–
  > æ¢¯åº¦åæ–¹å‘ï¼šå¾€å“ªä¸ªæ–¹å‘è°ƒæ•´å‚æ•°èƒ½è®©æŸå¤±å‡å°ã€‚
  > æ¢¯åº¦å¤§å°ï¼šè°ƒæ•´å¤šå°‘åˆé€‚ã€‚
- åå‘ä¼ æ’­ (Backward pass)ï¼šæŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°æ±‚å¯¼ï¼Œè®¡ç®—æ¢¯åº¦ã€‚
- æ¢¯åº¦ä¸‹é™æ³• (Gradient Descent)ï¼šæ–°å‚æ•°=æ—§å‚æ•°âˆ’ğœ‚â‹…æ¢¯åº¦
  > ğœ‚ æ˜¯ å­¦ä¹ ç‡ (learning rate)ï¼Œæ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•¿
ä»£ç ç¤ºä¾‹ï¼š
```py
import torch

x = torch.tensor([2.0], requires_grad=True)
y = x**2  # y = x^2
y.backward()  # è®¡ç®— dy/dx
print(x.grad)  # è¾“å‡º: tensor([4.])
```
## æ‰‹åŠ¨æ›´æ–°å‚æ•°
```py
import torch

# åˆå§‹åŒ–å‚æ•°
w = torch.tensor([1.0], requires_grad=True)
b = torch.tensor([0.5], requires_grad=True)
x = torch.tensor([2.0])
t = torch.tensor([10.0])
lr = 0.01

# å‰å‘ä¼ æ’­
y = (w * x + b)**2
loss = (y - t)**2

# åå‘ä¼ æ’­
loss.backward()

# æ›´æ–°å‚æ•°ï¼ˆæ‰‹åŠ¨ï¼‰
with torch.no_grad():
    w -= lr * w.grad
    b -= lr * b.grad

# æ¸…é™¤æ¢¯åº¦
w.grad.zero_()
b.grad.zero_()

```
## ä¼˜åŒ–å™¨ (Optimizer)æ›´æ–°å‚æ•°
```py
import torch
import torch.nn as nn
import torch.optim as optim

# å®šä¹‰ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹ y = wx + b
model = nn.Linear(1, 1)  

# æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·®
criterion = nn.MSELoss()

# ä¼˜åŒ–å™¨ï¼šéšæœºæ¢¯åº¦ä¸‹é™
optimizer = optim.SGD(model.parameters(), lr=0.1)

# è®­ç»ƒæ•°æ®ï¼šåªæœ‰ä¸€ç»„ (x=2, y=10)
x = torch.tensor([[2.0]])
y_true = torch.tensor([[10.0]])

# è¿­ä»£è®­ç»ƒ 10 æ¬¡
for epoch in range(10):
    # å‰å‘ä¼ æ’­
    y_pred = model(x)
    loss = criterion(y_pred, y_true)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()   # æ¸…ç©ºæ¢¯åº¦
    loss.backward()         # è®¡ç®—æ¢¯åº¦
    optimizer.step()        # æ›´æ–°å‚æ•°

    # æ‰“å°æƒé‡å’Œåç½®
    w = model.weight.data.item()
    b = model.bias.data.item()
    print(f"Epoch {epoch}: loss={loss.item():.4f}, weight={w:.4f}, bias={b:.4f}")
```
## å¤šå±‚ç¥ç»ç½‘ç»œ (MLP) æ‹ŸåˆäºŒæ¬¡å‡½æ•°
```py
import torch
import torch.nn as nn
import torch.optim as optim

# 1. ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼šy = 2x^2 + 3x + 1
x = torch.linspace(-5, 5, steps=200).unsqueeze(1)   # è¾“å…¥ç»´åº¦ [200,1]
y_true = 2 * x**2 + 3 * x + 1                       # è¾“å‡ºç»´åº¦ [200,1]

# 2. å®šä¹‰å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.hidden1 = nn.Linear(1, 16)   # è¾“å…¥å±‚ -> éšè—å±‚1
        self.hidden2 = nn.Linear(16, 16)  # éšè—å±‚1 -> éšè—å±‚2
        self.output = nn.Linear(16, 1)    # éšè—å±‚2 -> è¾“å‡ºå±‚
        self.relu = nn.ReLU()             # æ¿€æ´»å‡½æ•°

    def forward(self, x):
        x = self.relu(self.hidden1(x))    # ç¬¬ä¸€å±‚ + ReLU
        x = self.relu(self.hidden2(x))    # ç¬¬äºŒå±‚ + ReLU
        x = self.output(x)                # è¾“å‡ºå±‚ï¼ˆä¸åŠ æ¿€æ´»ï¼‰
        return x

# 3. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = MLP()
criterion = nn.MSELoss()                          # å‡æ–¹è¯¯å·®
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 4. è®­ç»ƒè¿‡ç¨‹
for epoch in range(200):
    y_pred = model(x)                             # å‰å‘ä¼ æ’­
    loss = criterion(y_pred, y_true)              # è®¡ç®—æŸå¤±

    optimizer.zero_grad()                         # æ¸…ç©ºæ¢¯åº¦
    loss.backward()                               # åå‘ä¼ æ’­
    optimizer.step()                              # æ›´æ–°å‚æ•°

    if epoch % 20 == 0:                           # æ¯20è½®æ‰“å°ä¸€æ¬¡
        print(f"Epoch {epoch:03d}: loss={loss.item():.4f}")
```
#### æ€»ç»“
1. **`nn.Linear` çš„çŸ©é˜µå’Œåç½®å¤§å°**  
   - æƒé‡çŸ©é˜µå½¢çŠ¶ï¼š\((out\_features, in\_features)\)  
   - åç½®å‘é‡å½¢çŠ¶ï¼š\((out\_features)\)ï¼Œåœ¨æ‰¹ç»´åº¦ä¸Šå¹¿æ’­æˆ \([batch\_size, out\_features]\)ã€‚  

2. **MLP æ‹ŸåˆäºŒæ¬¡å‡½æ•°çš„å®Œæ•´æµç¨‹**  
   - æ•°æ®å‡†å¤‡ï¼šç”Ÿæˆè¾“å…¥ \(x\) å’ŒçœŸå®è¾“å‡º \(y\)ã€‚  
   - æ¨¡å‹ç»“æ„ï¼šä¸¤å±‚éšè—å±‚ + ReLU æ¿€æ´»ï¼Œè¾“å‡ºå±‚ä¸åŠ æ¿€æ´»ã€‚  
   - æŸå¤±å‡½æ•°ï¼šMSEã€‚  
   - ä¼˜åŒ–å™¨ï¼šAdamã€‚  
   - è®­ç»ƒè¿‡ç¨‹ï¼šå‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°ã€‚  

3. **ReLU æ¿€æ´»å‡½æ•°çš„ä½œç”¨**  
   - å®šä¹‰ï¼š\(\text{ReLU}(x) = \max(0, x)\)ã€‚  
   - åœ¨éšè—å±‚é€ä¸ªä½œç”¨äºç¥ç»å…ƒè¾“å‡ºï¼Œå¼•å…¥éçº¿æ€§ã€‚  
   - è¾“å‡ºå±‚é€šå¸¸ä¸åŠ  ReLUï¼Œé¿å…é™åˆ¶è¾“å‡ºåªèƒ½ä¸ºæ­£æ•°ã€‚  

4. **æ¢¯åº¦ä¸‹é™ä¸ä¼˜åŒ–å™¨**  
   - æ¢¯åº¦æ˜¯æŸå¤±å‡½æ•°ä¸Šå‡æœ€å¿«çš„æ–¹å‘ã€‚  
   - æ›´æ–°å‚æ•°æ—¶å‡å»æ¢¯åº¦ï¼Œä¿è¯å¾€æŸå¤±ä¸‹é™çš„æ–¹å‘èµ°ã€‚  
   - Adam ä¼˜åŒ–å™¨ç»“åˆäº†åŠ¨é‡ï¼ˆå¹³æ»‘æ–¹å‘ï¼‰å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆä¸åŒå‚æ•°ä¸åŒæ­¥é•¿ï¼‰ã€‚  

5. **çŸ©é˜µç»´åº¦çš„å®Œæ•´è¿½è¸ª**  
   - è¾“å…¥ \([200,1]\) â†’ ç¬¬ä¸€å±‚ \([200,16]\) â†’ ç¬¬äºŒå±‚ \([200,16]\) â†’ è¾“å‡ºå±‚ \([200,1]\)ã€‚  
   - åç½®åœ¨å¹¿æ’­æ—¶ä» \([16]\) æ‰©å±•æˆ \([200,16]\)ã€‚  
