[Deep Learning with PyTorch: A 60 Minute Blitz](https://docs.pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)
<details>
  <summary>ç‚¹å‡»å±•å¼€ä»£ç å—</summary>

import torch
import numpy as np

#init
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)
print(x_data)

#np array
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
print(x_np)

#like
x_ones = torch.ones_like(x_data) # retains the properties of x_data
print(f"Ones like Tensor: \n {x_ones} \n")
#like and override
x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data
print(f"Random like Tensor: \n {x_rand} \n")

#shape
shape = (2, 3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print("shape:")
print(f"Random Tensor: \n {rand_tensor} ")
print(f"Ones Tensor: \n {ones_tensor} ")
print(f"Zeros Tensor: \n {zeros_tensor}")

</details>

# 1120 PyTorchå®è·µï¼šé€æ­¥è®¤è¯†MLPå¤šå±‚æ„ŸçŸ¥æœº
å…³é”®è¯ï¼š
å‰å‘ä¼ æ’­ã€æ¢¯åº¦ã€æŸå¤±å‡½æ•°ã€åå‘ä¼ æ’­ã€å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨ã€è¾“å…¥å±‚ã€éšè—å±‚ã€çº¿æ€§å˜æ¢ã€æ¿€æ´»å‡½æ•°ã€è¾“å‡ºå±‚ã€å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–ã€‚

[copilot:learn pytorch](https://copilot.microsoft.com/shares/gs2Dy6e3kvTPnzUwkvcjk)
## è®­ç»ƒç›¸å…³æ¦‚å¿µ
- å‰å‘ä¼ æ’­ (Forward pass)ï¼šè¾“å…¥ã€‹å¼ é‡è¿ç®—ã€‹è¾“å‡º
- æŸå¤±å‡½æ•°ï¼šæ¨¡å‹è¾“å‡ºå’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼Œæ¯”å¦‚å‡æ–¹è¯¯å·® (MSE)ã€äº¤å‰ç†µ (Cross-Entropy)ã€‚
  > æŸå¤±å‡½æ•° ğ¿ æ˜¯å‚æ•°çš„å‡½æ•°ï¼šğ¿=ğ‘“(ğ‘Š,ğ‘,ğ‘¥)
  >
  > å›å½’ä»»åŠ¡ â†’ MSE (å‡æ–¹è¯¯å·®)ã€‚  
  > åˆ†ç±»ä»»åŠ¡ â†’ CrossEntropyLossã€‚  
- æ¢¯åº¦ï¼šæŸå¤±å‡½æ•°å¯¹å‚æ•°çš„åå¯¼æ•°ã€‚å³ï¼šå¦‚æœå‚æ•°ğ‘Šæ”¹å˜ä¸€ç‚¹ç‚¹ï¼ŒæŸå¤±ğ¿ä¼šæ€ä¹ˆå˜åŒ–
  > æ¢¯åº¦åæ–¹å‘ï¼šå¾€å“ªä¸ªæ–¹å‘è°ƒæ•´å‚æ•°èƒ½è®©æŸå¤±å‡å°ã€‚
  > æ¢¯åº¦å¤§å°ï¼šè°ƒæ•´å¤šå°‘åˆé€‚ã€‚
- åå‘ä¼ æ’­ (Backward pass)ï¼šæŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°æ±‚å¯¼ï¼Œè®¡ç®—æ¢¯åº¦ï¼Œå¹¶æ›´æ–°å‚æ•°ã€‚
- æ¢¯åº¦ä¸‹é™æ³• (Gradient Descent)ï¼šæ–°å‚æ•°=æ—§å‚æ•°âˆ’ğœ‚â‹…æ¢¯åº¦
  > ğœ‚ æ˜¯ å­¦ä¹ ç‡ (learning rate)ï¼Œæ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•¿
ä»£ç ç¤ºä¾‹ï¼š
```py
import torch

x = torch.tensor([2.0], requires_grad=True)
y = x**2  # y = x^2
y.backward()  # è®¡ç®— dy/dx
print(x.grad)  # è¾“å‡º: tensor([4.])
```
## æ‰‹åŠ¨æ›´æ–°å‚æ•°
```py
import torch

# åˆå§‹åŒ–å‚æ•°
w = torch.tensor([1.0], requires_grad=True)
b = torch.tensor([0.5], requires_grad=True)
x = torch.tensor([2.0])
t = torch.tensor([10.0])
lr = 0.01

# å‰å‘ä¼ æ’­
y = (w * x + b)**2
loss = (y - t)**2

# åå‘ä¼ æ’­
loss.backward()

# æ›´æ–°å‚æ•°ï¼ˆæ‰‹åŠ¨ï¼‰
with torch.no_grad():
    w -= lr * w.grad
    b -= lr * b.grad

# æ¸…é™¤æ¢¯åº¦
w.grad.zero_()
b.grad.zero_()

```
## ä¼˜åŒ–å™¨ (Optimizer)æ›´æ–°å‚æ•°
```py
import torch
import torch.nn as nn
import torch.optim as optim

# å®šä¹‰ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹ y = wx + b
model = nn.Linear(1, 1)  

# æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·®
criterion = nn.MSELoss()

# ä¼˜åŒ–å™¨ï¼šéšæœºæ¢¯åº¦ä¸‹é™
optimizer = optim.SGD(model.parameters(), lr=0.1)

# è®­ç»ƒæ•°æ®ï¼šåªæœ‰ä¸€ç»„ (x=2, y=10)
x = torch.tensor([[2.0]])
y_true = torch.tensor([[10.0]])

# è¿­ä»£è®­ç»ƒ 10 æ¬¡
for epoch in range(10):
    # å‰å‘ä¼ æ’­
    y_pred = model(x)
    loss = criterion(y_pred, y_true)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()   # æ¸…ç©ºæ¢¯åº¦
    loss.backward()         # è®¡ç®—æ¢¯åº¦
    optimizer.step()        # æ›´æ–°å‚æ•°

    # æ‰“å°æƒé‡å’Œåç½®
    w = model.weight.data.item()
    b = model.bias.data.item()
    print(f"Epoch {epoch}: loss={loss.item():.4f}, weight={w:.4f}, bias={b:.4f}")
```
## MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰å…³é”®æ¦‚å¿µ

- **è¾“å…¥å±‚ (Input Layer)**  
  - æ¥æ”¶åŸå§‹æ•°æ®ï¼Œä¾‹å¦‚äºŒæ¬¡å‡½æ•°æ‹Ÿåˆä¸­çš„æ ‡é‡ \(x\)ï¼Œæˆ– MNIST ä¸­çš„ \(28 \times 28\) åƒç´ å›¾åƒã€‚  
  - å½¢çŠ¶é€šå¸¸æ˜¯ \([batch\_size, feature\_dim]\)ã€‚  

- **éšè—å±‚ (Hidden Layers)**  
  - ä½äºè¾“å…¥å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´ã€‚  
  - æ¯ä¸€å±‚ç”±è‹¥å¹²ç¥ç»å…ƒç»„æˆï¼Œè´Ÿè´£æå–ç‰¹å¾å’Œå¼•å…¥éçº¿æ€§ã€‚  
  - ç¤ºä¾‹ä¸­ä½¿ç”¨ä¸¤å±‚éšè—å±‚ï¼Œæ¯å±‚ 16 æˆ– 256 ä¸ªç¥ç»å…ƒã€‚  

- **çº¿æ€§å˜æ¢ (Linear Transformation)**  
  - æ¯ä¸ªç¥ç»å…ƒè®¡ç®—ï¼š
    \[
    z = W \cdot x + b
    \]
  - æƒé‡ \(W\) å’Œåç½® \(b\) æ˜¯éœ€è¦å­¦ä¹ çš„å‚æ•°ã€‚  

- **æ¿€æ´»å‡½æ•° (Activation Function)**  
  - å¼•å…¥éçº¿æ€§ï¼Œä½¿ç½‘ç»œèƒ½æ‹Ÿåˆå¤æ‚å‡½æ•°ã€‚  
  - ç¤ºä¾‹ä¸­ä½¿ç”¨ **ReLU**ï¼š  
    \[
    \text{ReLU}(x) = \max(0, x)
    \]  
  - è¾“å‡ºå±‚æ ¹æ®ä»»åŠ¡ä¸åŒé€‰æ‹©æ˜¯å¦åŠ æ¿€æ´»ï¼š  
    - å›å½’ä»»åŠ¡ â†’ ä¸åŠ æ¿€æ´»  
    - åˆ†ç±»ä»»åŠ¡ â†’ CrossEntropyLoss å†…éƒ¨åŒ…å« Softmax  

- **è¾“å‡ºå±‚ (Output Layer)**  
  - ç»™å‡ºæœ€ç»ˆé¢„æµ‹ç»“æœã€‚  
  - äºŒæ¬¡å‡½æ•°æ‹Ÿåˆ â†’ è¾“å‡ºä¸€ä¸ªæ ‡é‡ \(\hat{y}\)ã€‚  
  - MNIST åˆ†ç±» â†’ è¾“å‡º 10 ç»´å‘é‡ï¼Œå¯¹åº”æ•°å­— 0â€“9ã€‚  
## å¤šå±‚ç¥ç»ç½‘ç»œ (MLP) æ‹ŸåˆäºŒæ¬¡å‡½æ•°
```py
import torch
import torch.nn as nn
import torch.optim as optim

# 1. ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼šy = 2x^2 + 3x + 1
x = torch.linspace(-5, 5, steps=200).unsqueeze(1)   # è¾“å…¥ç»´åº¦ [200,1]
y_true = 2 * x**2 + 3 * x + 1                       # è¾“å‡ºç»´åº¦ [200,1]

# 2. å®šä¹‰å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.hidden1 = nn.Linear(1, 16)   # è¾“å…¥å±‚ -> éšè—å±‚1
        self.hidden2 = nn.Linear(16, 16)  # éšè—å±‚1 -> éšè—å±‚2
        self.output = nn.Linear(16, 1)    # éšè—å±‚2 -> è¾“å‡ºå±‚
        self.relu = nn.ReLU()             # æ¿€æ´»å‡½æ•°

    def forward(self, x):
        x = self.relu(self.hidden1(x))    # ç¬¬ä¸€å±‚ + ReLU
        x = self.relu(self.hidden2(x))    # ç¬¬äºŒå±‚ + ReLU
        x = self.output(x)                # è¾“å‡ºå±‚ï¼ˆä¸åŠ æ¿€æ´»ï¼‰
        return x

# 3. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = MLP()
criterion = nn.MSELoss()                          # å‡æ–¹è¯¯å·®
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 4. è®­ç»ƒè¿‡ç¨‹
for epoch in range(200):
    y_pred = model(x)                             # å‰å‘ä¼ æ’­
    loss = criterion(y_pred, y_true)              # è®¡ç®—æŸå¤±

    optimizer.zero_grad()                         # æ¸…ç©ºæ¢¯åº¦
    loss.backward()                               # åå‘ä¼ æ’­
    optimizer.step()                              # æ›´æ–°å‚æ•°

    if epoch % 20 == 0:                           # æ¯20è½®æ‰“å°ä¸€æ¬¡
        print(f"Epoch {epoch:03d}: loss={loss.item():.4f}")
```
1. **`nn.Linear` çš„çŸ©é˜µå’Œåç½®å¤§å°**  
   - æƒé‡çŸ©é˜µå½¢çŠ¶ï¼š\((out\_features, in\_features)\)  
   - åç½®å‘é‡å½¢çŠ¶ï¼š\((out\_features)\)ï¼Œåœ¨æ‰¹ç»´åº¦ä¸Šå¹¿æ’­æˆ \([batch\_size, out\_features]\)ã€‚  

2. **MLP æ‹ŸåˆäºŒæ¬¡å‡½æ•°çš„å®Œæ•´æµç¨‹**  
   - æ•°æ®å‡†å¤‡ï¼šç”Ÿæˆè¾“å…¥ \(x\) å’ŒçœŸå®è¾“å‡º \(y\)ã€‚  
   - æ¨¡å‹ç»“æ„ï¼šä¸¤å±‚éšè—å±‚ + ReLU æ¿€æ´»ï¼Œè¾“å‡ºå±‚ä¸åŠ æ¿€æ´»ã€‚  
   - æŸå¤±å‡½æ•°ï¼šMSEã€‚  
   - ä¼˜åŒ–å™¨ï¼šAdamã€‚  
   - è®­ç»ƒè¿‡ç¨‹ï¼šå‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°ã€‚  

3. **ReLU æ¿€æ´»å‡½æ•°çš„ä½œç”¨**  
   - å®šä¹‰ï¼š\(\text{ReLU}(x) = \max(0, x)\)ã€‚  
   - åœ¨éšè—å±‚é€ä¸ªä½œç”¨äºç¥ç»å…ƒè¾“å‡ºï¼Œå¼•å…¥éçº¿æ€§ã€‚  
   - è¾“å‡ºå±‚é€šå¸¸ä¸åŠ  ReLUï¼Œé¿å…é™åˆ¶è¾“å‡ºåªèƒ½ä¸ºæ­£æ•°ã€‚  

4. **æ¢¯åº¦ä¸‹é™ä¸ä¼˜åŒ–å™¨**  
   - æ¢¯åº¦æ˜¯æŸå¤±å‡½æ•°ä¸Šå‡æœ€å¿«çš„æ–¹å‘ã€‚  
   - æ›´æ–°å‚æ•°æ—¶å‡å»æ¢¯åº¦ï¼Œä¿è¯å¾€æŸå¤±ä¸‹é™çš„æ–¹å‘èµ°ã€‚  
   - Adam ä¼˜åŒ–å™¨ç»“åˆäº†åŠ¨é‡ï¼ˆå¹³æ»‘æ–¹å‘ï¼‰å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆä¸åŒå‚æ•°ä¸åŒæ­¥é•¿ï¼‰ã€‚  

5. **çŸ©é˜µç»´åº¦çš„å®Œæ•´è¿½è¸ª**  
   - è¾“å…¥ \([200,1]\) â†’ ç¬¬ä¸€å±‚ \([200,16]\) â†’ ç¬¬äºŒå±‚ \([200,16]\) â†’ è¾“å‡ºå±‚ \([200,1]\)ã€‚  
   - åç½®åœ¨å¹¿æ’­æ—¶ä» \([16]\) æ‰©å±•æˆ \([200,16]\)ã€‚
  
## æ‰©å±•ï¼šMLP è¯†åˆ« MNIST æ‰‹å†™æ•°å­—
```py
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# 1. åŠ è½½ MNIST æ•°æ®é›†
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# 2. å®šä¹‰ MLP æ¨¡å‹
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(28*28, 256)   # è¾“å…¥å±‚ -> éšè—å±‚1
        self.fc2 = nn.Linear(256, 128)     # éšè—å±‚1 -> éšè—å±‚2
        self.fc3 = nn.Linear(128, 10)      # éšè—å±‚2 -> è¾“å‡ºå±‚ (10ç±»)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(-1, 28*28)              # å±•å¹³å›¾åƒ [batch, 784]
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)                    # è¾“å‡ºå±‚ä¸åŠ æ¿€æ´»ï¼Œäº¤ç»™ CrossEntropyLoss
        return x

model = MLP()

# 3. æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()          # åˆ†ç±»ä»»åŠ¡å¸¸ç”¨æŸå¤±å‡½æ•°
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. è®­ç»ƒæ¨¡å‹
for epoch in range(5):                     # è®­ç»ƒ5ä¸ªepoch
    for images, labels in train_loader:
        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}")

# 5. æµ‹è¯•æ¨¡å‹
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"Test Accuracy: {100 * correct / total:.2f}%")
```
